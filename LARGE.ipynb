{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c6d5dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.3.0)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.5.1%2Bcpu-cp310-cp310-linux_x86_64.whl (174.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 MB\u001b[0m \u001b[31m182.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.20.1%2Bcpu-cp310-cp310-linux_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.5.1%2Bcpu-cp310-cp310-linux_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.4)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m184.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: sympy, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.3\n",
      "    Uninstalling sympy-1.13.3:\n",
      "      Successfully uninstalled sympy-1.13.3\n",
      "Successfully installed sympy-1.13.1 torch-2.5.1+cpu torchaudio-2.5.1+cpu torchvision-0.20.1+cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da644c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"FoodClassification\") \\\n",
    "#     .config(\"spark.executor.memory\", \"4g\") \\\n",
    "#     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#     .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d474a0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 4) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.range(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af8e6183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import boto3\n",
    "import os\n",
    "from pathlib import Path\n",
    "import s3fs\n",
    "import boto3\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cccc47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS S3 Bucket Configuration\n",
    "s3_bucket = 'bdafoodimages-unzipped'\n",
    "train_folder = 'dataset/train'\n",
    "val_folder = 'dataset/val'\n",
    "test_folder = 'dataset/test'\n",
    "\n",
    "\n",
    "# Initialize S3 FileSystem to access the bucket\n",
    "fs = s3fs.S3FileSystem()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9698761",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3', region_name='us-east-1')  # Specify the correct region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e4234cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in S3 folder (returns image paths in the folder)\n",
    "def list_files_in_s3(bucket_name, folder_path):\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder_path)\n",
    "    if 'Contents' in response:\n",
    "        return [content['Key'] for content in response['Contents']]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Fetch image paths from S3\n",
    "def fetch_images_from_s3(bucket_name, folder_path):\n",
    "    files = list_files_in_s3(bucket_name, folder_path)\n",
    "    images = []\n",
    "    for file in files:\n",
    "        if file.endswith(('jpg', 'jpeg', 'png')):  # Filter only image files\n",
    "            images.append(file)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01924225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize image to 128x128\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize as ImageNet\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fadd9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset to load images from S3\n",
    "class S3ImageDataset(Dataset):\n",
    "    def __init__(self, bucket_name, folder_path, transform=None):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.folder_path = folder_path\n",
    "        self.files = fetch_images_from_s3(bucket_name, folder_path)  # List of image paths\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(sorted(set([file.split('/')[2] for file in self.files])))}\n",
    "        self.idx_to_class = {idx: class_name for class_name, idx in self.class_to_idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the S3 object key\n",
    "        file_path = self.files[idx]\n",
    "        \n",
    "        # Download the image from S3 to memory\n",
    "        response = s3_client.get_object(Bucket=self.bucket_name, Key=file_path)\n",
    "        img_data = response['Body'].read()\n",
    "        \n",
    "        # Open image using PIL\n",
    "        img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "        # Extract the label from the file path (folder name is the label)\n",
    "        label_str = file_path.split('/')[2]  # Modify this based on your folder structure\n",
    "        label = self.class_to_idx[label_str]  # Convert label to index\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1db248c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader instances for training, validation, and testing\n",
    "train_dataset = S3ImageDataset(s3_bucket, train_folder, transform)\n",
    "val_dataset = S3ImageDataset(s3_bucket, val_folder, transform)\n",
    "test_dataset = S3ImageDataset(s3_bucket, test_folder, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "541abb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input shape: torch.Size([32, 3, 128, 128]), Labels: tensor([1, 2, 2, 0, 0, 1, 1, 2, 2, 1, 0, 1, 0, 2, 0, 1, 0, 2, 1, 2, 2, 0, 1, 1,\n",
      "        0, 1, 0, 2, 1, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Create DataLoaders for batching and shuffling\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Debugging: Check the first batch\n",
    "for inputs, labels in train_loader:\n",
    "    print(f\"Batch input shape: {inputs.shape}, Labels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc9dbc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim \n",
    "\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Corrected input size for fc1\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleCNN(num_classes=3)  # Adjust the number of classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0298aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (CrossEntropyLoss for multi-class classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Adam optimizer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28ccf49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3.7745, Accuracy: 33.54%\n",
      "Epoch 2/10, Loss: 1.0693, Accuracy: 44.65%\n",
      "Epoch 3/10, Loss: 0.8110, Accuracy: 59.40%\n",
      "Epoch 4/10, Loss: 0.5362, Accuracy: 77.78%\n",
      "Epoch 5/10, Loss: 0.2923, Accuracy: 90.13%\n",
      "Epoch 6/10, Loss: 0.1537, Accuracy: 95.85%\n",
      "Epoch 7/10, Loss: 0.1376, Accuracy: 96.16%\n",
      "Epoch 8/10, Loss: 0.0541, Accuracy: 99.17%\n",
      "Epoch 9/10, Loss: 0.0479, Accuracy: 99.07%\n",
      "Epoch 10/10, Loss: 0.0354, Accuracy: 99.27%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the device (GPU/CPU)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10  # Set the number of epochs based on your preference\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move data to the device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss and accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get the predictions from the model\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Track correct predictions and total predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate loss and accuracy for this epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "\n",
    "    # Print loss and accuracy for this epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "    # Add validation code here if desired\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a66f9a7c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.98      0.20        49\n",
      "           1       0.15      0.84      0.25        45\n",
      "           2       0.17      0.92      0.29        50\n",
      "           3       0.00      0.00      0.00        48\n",
      "           4       0.00      0.00      0.00        50\n",
      "           5       0.00      0.00      0.00        42\n",
      "           6       0.00      0.00      0.00        48\n",
      "           7       0.00      0.00      0.00        49\n",
      "           8       0.00      0.00      0.00        50\n",
      "           9       0.00      0.00      0.00        49\n",
      "          10       0.00      0.00      0.00        46\n",
      "          11       0.00      0.00      0.00        47\n",
      "          12       0.00      0.00      0.00        49\n",
      "          13       0.00      0.00      0.00        47\n",
      "          14       0.00      0.00      0.00        49\n",
      "          15       0.00      0.00      0.00        50\n",
      "          16       0.00      0.00      0.00        50\n",
      "          17       0.00      0.00      0.00        17\n",
      "          18       0.00      0.00      0.00        47\n",
      "          19       0.00      0.00      0.00        49\n",
      "          20       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.14       962\n",
      "   macro avg       0.02      0.13      0.04       962\n",
      "weighted avg       0.02      0.14      0.04       962\n",
      "\n",
      "Confusion Matrix:\n",
      " [[48  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4 38  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  3 46  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [22 11 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [32 10  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [11 21 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [21 13 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [26 11 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [21 14 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [28 11 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [21 15 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [19  8 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [19 20 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [23 12 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [22 11 16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [27  9 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [18 17 15  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 9  2  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [16 17 14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [28 14  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [15  4 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Gather all true labels and predictions for test data\n",
    "true_labels = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labe\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a9ea6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 13.72%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to track the number of correct predictions and total samples\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient calculation during evaluation for efficiency\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move data to the device (GPU or CPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Get the predicted class by choosing the class with the maximum output probability\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Track the number of correct predictions and the total number of samples\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "# Print the accuracy on the test set\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f005e44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to food_image_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model_save_path = 'food_image_classifier.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bf3ea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to S3 at s3://bdafoodimages-unzipped/models/food_image_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "# Upload the saved model to S3\n",
    "model_s3_path = 'models/food_image_classifier.pth'  # Specify the folder and file name in S3\n",
    "s3_client.upload_file(model_save_path, s3_bucket, model_s3_path)\n",
    "print(f\"Model uploaded to S3 at s3://{s3_bucket}/{model_s3_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f337671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9416/3994405318.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_load_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model_load_path = 'food_image_classifier.pth'  # Path where the model is saved locally or download from S3\n",
    "model = SimpleCNN(num_classes=len(train_dataset.class_to_idx))\n",
    "model.load_state_dict(torch.load(model_load_path))\n",
    "model.to(device)\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "497d5acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Alu_Gobi\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import io\n",
    "import boto3\n",
    "\n",
    "# AWS S3 Bucket Configuration\n",
    "s3_bucket = 'bdafoodimages-unzipped'\n",
    "image_key = 'dataset/val/Alu_Matar/Image_108.jpg'  # Path in your S3 bucket\n",
    "\n",
    "# Fetch the image from S3\n",
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.get_object(Bucket=s3_bucket, Key=image_key)\n",
    "img_data = response['Body'].read()\n",
    "\n",
    "# Open the image using PIL\n",
    "img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "# Define the image transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Preprocess the image\n",
    "img = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move to the appropriate device\n",
    "img = img.to(device)\n",
    "\n",
    "# Perform inference\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(img)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = train_dataset.idx_to_class[predicted.item()]  # Use your class mapping\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80229d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd624e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6904daa4",
   "metadata": {},
   "source": [
    "# STREAMLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "435c6087",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.40.1)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (5.4.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (1.8.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (21.3)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (1.5.3)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (11.0.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (4.25.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (17.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (13.9.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from streamlit) (5.0.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (1.13.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging<25,>=20->streamlit) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<3,>=1.4.0->streamlit) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b788349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Streamlit app script (app.py)\n",
    "streamlit_code = \"\"\"\n",
    "import streamlit as st\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "\n",
    "# Define your model architecture (it should be the same as the model you trained)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=300):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 64 * 64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 64 * 64)  # Flatten the tensor for the fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Load the saved model\n",
    "model = SimpleCNN(num_classes=300)\n",
    "model.load_state_dict(torch.load('food_image_classifier.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Define the transformation used for the image during inference\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Streamlit UI elements\n",
    "st.title('Food Image Classifier')\n",
    "st.write(\"Upload an image to classify.\")\n",
    "\n",
    "# File uploader\n",
    "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "\n",
    "if uploaded_file is not None:\n",
    "    # Load and display the image\n",
    "    image = Image.open(uploaded_file)\n",
    "    st.image(image, caption='Uploaded Image.', use_column_width=True)\n",
    "    st.write(\"\")\n",
    "    \n",
    "    # Preprocess the image\n",
    "    img_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    img_tensor = img_tensor.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "    predicted_class = f\"Predicted Class: {predicted.item()}\"  # You can map this to actual class names\n",
    "    st.write(predicted_class)\n",
    "\"\"\"\n",
    "\n",
    "# Save the Streamlit app to a file\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(streamlit_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e2edd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"streamlit_app.py\", \"w\") as f:\n",
    "    f.write(streamlit_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce2280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef75ed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2-user 18287  0.0  0.0 119860  2732 pts/0    Ss+  11:05   0:00 /bin/sh -c ps aux | grep streamlit\r\n",
      "ec2-user 18295  0.0  0.0 119420   928 pts/0    S+   11:05   0:00 grep streamlit\r\n"
     ]
    }
   ],
   "source": [
    "!ps aux | grep streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb02ef1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
